{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CQhzn5_AAdLj"
   },
   "source": [
    "#### Required Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R_GiTKWEdQR2"
   },
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa  # package for speech and audio analysis\n",
    "from sklearn import mixture\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from scipy.io import loadmat\n",
    "from scipy.io import savemat\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pylab as pl\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "%config InlineBackend.figure_format = 'retina'   ##QUALITY FIGURES!!\n",
    "import time\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CHqnQDkTfqBk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V-iosdeje0xk"
   },
   "outputs": [],
   "source": [
    "XTrain = pd.read_csv('x_trainset_clean.csv', error_bad_lines=False)\n",
    "XTest = pd.read_csv('x_testset_clean.csv', sep=\";\", error_bad_lines=False)\n",
    "YTrain = pd.read_csv('y_trainset_clean.csv', error_bad_lines=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 797
    },
    "colab_type": "code",
    "id": "7tOyS4D2f1EU",
    "outputId": "a516c205-ced6-4fef-9b17-716ef2a470b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        feat_0   feat_1   feat_2  ...  feat_144  feat_145  feat_146\n",
      "0      15.1368  22.0563  13.8909  ...   10.5976    2.4412    7.8157\n",
      "1      17.8086   6.7716   7.6309  ...   13.4864    5.5657   34.4324\n",
      "2      16.6050   8.9661   5.2370  ...   14.1878    0.8427   14.5175\n",
      "3      17.9498  21.0946  11.8534  ...   11.9454   -0.6505   32.5443\n",
      "4      11.4204  23.7611   3.7186  ...   14.5033   10.0882   18.1030\n",
      "...        ...      ...      ...  ...       ...       ...       ...\n",
      "89411  20.9624  12.0519   5.5867  ...    7.8009    9.0138    5.8544\n",
      "89412   1.6635  13.0759   3.8412  ...   10.5684   -3.0129   14.6587\n",
      "89413  12.5236  24.9418  11.5404  ...   11.6344    5.8713   29.0477\n",
      "89414   3.7362   2.2806   5.1252  ...    6.3111   -2.8103   11.0105\n",
      "89415   6.0730  13.2655  10.4632  ...   10.6917    0.6781   27.1135\n",
      "\n",
      "[89416 rows x 119 columns]\n",
      "       target\n",
      "0           0\n",
      "1           0\n",
      "2           0\n",
      "3           0\n",
      "4           0\n",
      "...       ...\n",
      "89411       1\n",
      "89412       0\n",
      "89413       0\n",
      "89414       0\n",
      "89415       0\n",
      "\n",
      "[89416 rows x 1 columns]\n",
      "        feat_0   feat_1   feat_2  ...  feat_144  feat_145  feat_146\n",
      "0      19.2344   6.7956  14.2244  ...   16.9051   -0.7666   11.7494\n",
      "1       7.0573  17.2379   8.2168  ...   17.3274   11.2564   13.3718\n",
      "2      16.3194   6.2243   7.3876  ...    6.9075    1.8037   17.9785\n",
      "3       5.6022   1.3580  12.3035  ...    9.9056    6.8607   10.2752\n",
      "4      23.9358   7.6822  13.0119  ...    7.3357    2.2630   25.5609\n",
      "...        ...      ...      ...  ...       ...       ...       ...\n",
      "44037   2.1462   6.2249   5.7545  ...    9.9586    4.8031   12.6960\n",
      "44038  19.0973  17.7384  10.9023  ...   12.3234    7.9111   19.0663\n",
      "44039  23.6109  31.2774  10.2917  ...   11.1035    4.6569   18.3415\n",
      "44040  -1.8853  13.9992   5.5941  ...    8.8483    2.5707   19.3350\n",
      "44041   5.9782  31.5648  15.0847  ...   13.1494    5.5051    8.0765\n",
      "\n",
      "[44042 rows x 119 columns]\n",
      "(89416, 119)\n",
      "(89416, 1)\n",
      "(44042, 119)\n"
     ]
    }
   ],
   "source": [
    "print(XTrain)\n",
    "print(YTrain)\n",
    "print(XTest)\n",
    "\n",
    "print(XTrain.shape)\n",
    "print(YTrain.shape)\n",
    "print(XTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "colab_type": "code",
    "id": "io8JOEUpgWp5",
    "outputId": "17ea7ede-60f5-40d6-d59a-b8fc37f31ff1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>feat_10</th>\n",
       "      <th>feat_12</th>\n",
       "      <th>feat_13</th>\n",
       "      <th>feat_14</th>\n",
       "      <th>feat_15</th>\n",
       "      <th>feat_18</th>\n",
       "      <th>feat_19</th>\n",
       "      <th>feat_21</th>\n",
       "      <th>feat_22</th>\n",
       "      <th>feat_23</th>\n",
       "      <th>feat_24</th>\n",
       "      <th>feat_25</th>\n",
       "      <th>feat_27</th>\n",
       "      <th>feat_28</th>\n",
       "      <th>feat_29</th>\n",
       "      <th>feat_30</th>\n",
       "      <th>feat_31</th>\n",
       "      <th>feat_32</th>\n",
       "      <th>feat_33</th>\n",
       "      <th>feat_34</th>\n",
       "      <th>feat_35</th>\n",
       "      <th>feat_36</th>\n",
       "      <th>feat_37</th>\n",
       "      <th>feat_38</th>\n",
       "      <th>feat_39</th>\n",
       "      <th>feat_41</th>\n",
       "      <th>feat_42</th>\n",
       "      <th>feat_43</th>\n",
       "      <th>feat_45</th>\n",
       "      <th>feat_46</th>\n",
       "      <th>feat_47</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_99</th>\n",
       "      <th>feat_100</th>\n",
       "      <th>feat_101</th>\n",
       "      <th>feat_103</th>\n",
       "      <th>feat_104</th>\n",
       "      <th>feat_105</th>\n",
       "      <th>feat_106</th>\n",
       "      <th>feat_107</th>\n",
       "      <th>feat_108</th>\n",
       "      <th>feat_109</th>\n",
       "      <th>feat_111</th>\n",
       "      <th>feat_113</th>\n",
       "      <th>feat_114</th>\n",
       "      <th>feat_115</th>\n",
       "      <th>feat_116</th>\n",
       "      <th>feat_119</th>\n",
       "      <th>feat_120</th>\n",
       "      <th>feat_122</th>\n",
       "      <th>feat_123</th>\n",
       "      <th>feat_124</th>\n",
       "      <th>feat_125</th>\n",
       "      <th>feat_126</th>\n",
       "      <th>feat_127</th>\n",
       "      <th>feat_128</th>\n",
       "      <th>feat_130</th>\n",
       "      <th>feat_131</th>\n",
       "      <th>feat_132</th>\n",
       "      <th>feat_133</th>\n",
       "      <th>feat_134</th>\n",
       "      <th>feat_135</th>\n",
       "      <th>feat_136</th>\n",
       "      <th>feat_138</th>\n",
       "      <th>feat_139</th>\n",
       "      <th>feat_140</th>\n",
       "      <th>feat_141</th>\n",
       "      <th>feat_142</th>\n",
       "      <th>feat_143</th>\n",
       "      <th>feat_144</th>\n",
       "      <th>feat_145</th>\n",
       "      <th>feat_146</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.1368</td>\n",
       "      <td>22.0563</td>\n",
       "      <td>13.8909</td>\n",
       "      <td>-0.191195</td>\n",
       "      <td>7.9553</td>\n",
       "      <td>-1.7273</td>\n",
       "      <td>12.187799</td>\n",
       "      <td>9.3627</td>\n",
       "      <td>25.235933</td>\n",
       "      <td>5.0502</td>\n",
       "      <td>34.3770</td>\n",
       "      <td>7.292606</td>\n",
       "      <td>5.3294</td>\n",
       "      <td>20.0104</td>\n",
       "      <td>0.3231</td>\n",
       "      <td>-4.3572</td>\n",
       "      <td>5.2440</td>\n",
       "      <td>20.198046</td>\n",
       "      <td>2.0361</td>\n",
       "      <td>-1.8996</td>\n",
       "      <td>10.8160</td>\n",
       "      <td>13.9493</td>\n",
       "      <td>21.4132</td>\n",
       "      <td>8.1024</td>\n",
       "      <td>9.226849</td>\n",
       "      <td>2.7611</td>\n",
       "      <td>2.3705</td>\n",
       "      <td>-3.458827</td>\n",
       "      <td>1.7545</td>\n",
       "      <td>22.9599</td>\n",
       "      <td>18.6433</td>\n",
       "      <td>9.6749</td>\n",
       "      <td>1.073915</td>\n",
       "      <td>2.2864</td>\n",
       "      <td>-5.0821</td>\n",
       "      <td>6.6609</td>\n",
       "      <td>17.6417</td>\n",
       "      <td>-4.8660</td>\n",
       "      <td>25.235858</td>\n",
       "      <td>10.487520</td>\n",
       "      <td>...</td>\n",
       "      <td>4.505490</td>\n",
       "      <td>-2.750010</td>\n",
       "      <td>4.119246</td>\n",
       "      <td>0.080186</td>\n",
       "      <td>2.1061</td>\n",
       "      <td>5.399134</td>\n",
       "      <td>17.8306</td>\n",
       "      <td>-2.247355</td>\n",
       "      <td>-0.2146</td>\n",
       "      <td>9.844442</td>\n",
       "      <td>4.566804</td>\n",
       "      <td>8.238292</td>\n",
       "      <td>-8.7572</td>\n",
       "      <td>3.021113</td>\n",
       "      <td>16.819486</td>\n",
       "      <td>-11.9314</td>\n",
       "      <td>3.706577</td>\n",
       "      <td>29.3683</td>\n",
       "      <td>-19.8745</td>\n",
       "      <td>-9.5051</td>\n",
       "      <td>-0.1658</td>\n",
       "      <td>11.1666</td>\n",
       "      <td>11.8424</td>\n",
       "      <td>8.9754</td>\n",
       "      <td>-1.1784</td>\n",
       "      <td>-6.7446</td>\n",
       "      <td>23.588553</td>\n",
       "      <td>-6.997934</td>\n",
       "      <td>12.0162</td>\n",
       "      <td>8.398301</td>\n",
       "      <td>1.364990</td>\n",
       "      <td>2.818466</td>\n",
       "      <td>-10.2904</td>\n",
       "      <td>21.4604</td>\n",
       "      <td>19.033370</td>\n",
       "      <td>5.6290</td>\n",
       "      <td>11.4794</td>\n",
       "      <td>10.5976</td>\n",
       "      <td>2.4412</td>\n",
       "      <td>7.8157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17.8086</td>\n",
       "      <td>6.7716</td>\n",
       "      <td>7.6309</td>\n",
       "      <td>-0.055241</td>\n",
       "      <td>11.4959</td>\n",
       "      <td>1.4697</td>\n",
       "      <td>9.162813</td>\n",
       "      <td>9.6937</td>\n",
       "      <td>13.064266</td>\n",
       "      <td>4.4119</td>\n",
       "      <td>13.1720</td>\n",
       "      <td>-4.065320</td>\n",
       "      <td>5.2186</td>\n",
       "      <td>21.0644</td>\n",
       "      <td>-1.0444</td>\n",
       "      <td>2.4304</td>\n",
       "      <td>6.7268</td>\n",
       "      <td>25.131809</td>\n",
       "      <td>-0.3916</td>\n",
       "      <td>2.0942</td>\n",
       "      <td>11.8662</td>\n",
       "      <td>14.1563</td>\n",
       "      <td>6.5568</td>\n",
       "      <td>6.6152</td>\n",
       "      <td>12.883735</td>\n",
       "      <td>-3.5547</td>\n",
       "      <td>-1.2882</td>\n",
       "      <td>6.960783</td>\n",
       "      <td>0.4536</td>\n",
       "      <td>10.0470</td>\n",
       "      <td>18.0010</td>\n",
       "      <td>10.9927</td>\n",
       "      <td>1.814642</td>\n",
       "      <td>13.8567</td>\n",
       "      <td>-3.3802</td>\n",
       "      <td>7.1529</td>\n",
       "      <td>23.8909</td>\n",
       "      <td>-5.0054</td>\n",
       "      <td>11.145854</td>\n",
       "      <td>14.562684</td>\n",
       "      <td>...</td>\n",
       "      <td>9.367697</td>\n",
       "      <td>1.463411</td>\n",
       "      <td>-0.797441</td>\n",
       "      <td>0.046634</td>\n",
       "      <td>10.0837</td>\n",
       "      <td>8.347122</td>\n",
       "      <td>15.0473</td>\n",
       "      <td>-4.588342</td>\n",
       "      <td>3.1251</td>\n",
       "      <td>9.388759</td>\n",
       "      <td>0.183109</td>\n",
       "      <td>7.272796</td>\n",
       "      <td>4.5436</td>\n",
       "      <td>2.070296</td>\n",
       "      <td>8.523278</td>\n",
       "      <td>-3.0338</td>\n",
       "      <td>3.601683</td>\n",
       "      <td>31.5370</td>\n",
       "      <td>-3.9798</td>\n",
       "      <td>10.6923</td>\n",
       "      <td>-1.7238</td>\n",
       "      <td>16.1436</td>\n",
       "      <td>12.3653</td>\n",
       "      <td>15.4920</td>\n",
       "      <td>0.3449</td>\n",
       "      <td>-3.6219</td>\n",
       "      <td>19.723483</td>\n",
       "      <td>4.745728</td>\n",
       "      <td>12.6425</td>\n",
       "      <td>6.198910</td>\n",
       "      <td>3.855401</td>\n",
       "      <td>7.774804</td>\n",
       "      <td>-29.9966</td>\n",
       "      <td>14.5911</td>\n",
       "      <td>8.202287</td>\n",
       "      <td>11.6062</td>\n",
       "      <td>10.4924</td>\n",
       "      <td>13.4864</td>\n",
       "      <td>5.5657</td>\n",
       "      <td>34.4324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.6050</td>\n",
       "      <td>8.9661</td>\n",
       "      <td>5.2370</td>\n",
       "      <td>-0.645798</td>\n",
       "      <td>11.4191</td>\n",
       "      <td>-1.4950</td>\n",
       "      <td>14.591945</td>\n",
       "      <td>10.5820</td>\n",
       "      <td>22.054072</td>\n",
       "      <td>5.7008</td>\n",
       "      <td>44.7409</td>\n",
       "      <td>-1.826598</td>\n",
       "      <td>4.5699</td>\n",
       "      <td>18.7247</td>\n",
       "      <td>8.4312</td>\n",
       "      <td>0.8081</td>\n",
       "      <td>-2.1199</td>\n",
       "      <td>23.713941</td>\n",
       "      <td>-3.1966</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>11.7777</td>\n",
       "      <td>13.9805</td>\n",
       "      <td>22.9775</td>\n",
       "      <td>10.8050</td>\n",
       "      <td>6.906523</td>\n",
       "      <td>4.1278</td>\n",
       "      <td>-1.2150</td>\n",
       "      <td>-2.533364</td>\n",
       "      <td>1.0484</td>\n",
       "      <td>7.0125</td>\n",
       "      <td>11.1446</td>\n",
       "      <td>10.7565</td>\n",
       "      <td>-0.818204</td>\n",
       "      <td>6.8730</td>\n",
       "      <td>-1.5589</td>\n",
       "      <td>7.0722</td>\n",
       "      <td>22.9657</td>\n",
       "      <td>-9.3820</td>\n",
       "      <td>17.645019</td>\n",
       "      <td>12.173638</td>\n",
       "      <td>...</td>\n",
       "      <td>1.934635</td>\n",
       "      <td>-5.541413</td>\n",
       "      <td>5.767185</td>\n",
       "      <td>4.591863</td>\n",
       "      <td>6.7978</td>\n",
       "      <td>9.600401</td>\n",
       "      <td>5.7450</td>\n",
       "      <td>-2.837220</td>\n",
       "      <td>2.3697</td>\n",
       "      <td>9.873878</td>\n",
       "      <td>-0.320446</td>\n",
       "      <td>5.539234</td>\n",
       "      <td>-7.7650</td>\n",
       "      <td>6.274574</td>\n",
       "      <td>11.449686</td>\n",
       "      <td>1.6084</td>\n",
       "      <td>3.235572</td>\n",
       "      <td>14.1648</td>\n",
       "      <td>-13.6226</td>\n",
       "      <td>-4.3668</td>\n",
       "      <td>-3.0475</td>\n",
       "      <td>21.2072</td>\n",
       "      <td>11.7247</td>\n",
       "      <td>12.2308</td>\n",
       "      <td>-2.0697</td>\n",
       "      <td>4.7666</td>\n",
       "      <td>20.108064</td>\n",
       "      <td>4.675584</td>\n",
       "      <td>11.9461</td>\n",
       "      <td>4.487912</td>\n",
       "      <td>6.542435</td>\n",
       "      <td>3.333723</td>\n",
       "      <td>-17.2859</td>\n",
       "      <td>17.9095</td>\n",
       "      <td>19.069860</td>\n",
       "      <td>2.1216</td>\n",
       "      <td>12.4473</td>\n",
       "      <td>14.1878</td>\n",
       "      <td>0.8427</td>\n",
       "      <td>14.5175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.9498</td>\n",
       "      <td>21.0946</td>\n",
       "      <td>11.8534</td>\n",
       "      <td>-0.419281</td>\n",
       "      <td>13.1532</td>\n",
       "      <td>0.3122</td>\n",
       "      <td>12.731332</td>\n",
       "      <td>9.1938</td>\n",
       "      <td>26.656922</td>\n",
       "      <td>7.6711</td>\n",
       "      <td>15.2589</td>\n",
       "      <td>-1.245958</td>\n",
       "      <td>6.4331</td>\n",
       "      <td>22.1703</td>\n",
       "      <td>6.7182</td>\n",
       "      <td>-0.3318</td>\n",
       "      <td>2.0474</td>\n",
       "      <td>25.735372</td>\n",
       "      <td>7.8308</td>\n",
       "      <td>0.1848</td>\n",
       "      <td>11.4201</td>\n",
       "      <td>14.0319</td>\n",
       "      <td>17.9287</td>\n",
       "      <td>10.1657</td>\n",
       "      <td>13.039255</td>\n",
       "      <td>3.1793</td>\n",
       "      <td>9.1255</td>\n",
       "      <td>5.804002</td>\n",
       "      <td>1.6957</td>\n",
       "      <td>19.1405</td>\n",
       "      <td>17.0947</td>\n",
       "      <td>10.8352</td>\n",
       "      <td>-2.018205</td>\n",
       "      <td>10.9129</td>\n",
       "      <td>-16.8299</td>\n",
       "      <td>5.9460</td>\n",
       "      <td>13.4046</td>\n",
       "      <td>-9.0359</td>\n",
       "      <td>22.626716</td>\n",
       "      <td>16.743269</td>\n",
       "      <td>...</td>\n",
       "      <td>10.093160</td>\n",
       "      <td>-3.576497</td>\n",
       "      <td>7.071735</td>\n",
       "      <td>-1.970255</td>\n",
       "      <td>1.1569</td>\n",
       "      <td>9.165919</td>\n",
       "      <td>13.2308</td>\n",
       "      <td>-3.458333</td>\n",
       "      <td>4.1654</td>\n",
       "      <td>17.026603</td>\n",
       "      <td>3.605175</td>\n",
       "      <td>4.954112</td>\n",
       "      <td>-7.9461</td>\n",
       "      <td>3.137077</td>\n",
       "      <td>13.484983</td>\n",
       "      <td>-1.6661</td>\n",
       "      <td>3.998095</td>\n",
       "      <td>3.5454</td>\n",
       "      <td>-6.1492</td>\n",
       "      <td>-0.6783</td>\n",
       "      <td>1.4033</td>\n",
       "      <td>17.8143</td>\n",
       "      <td>12.1084</td>\n",
       "      <td>16.6800</td>\n",
       "      <td>-4.5560</td>\n",
       "      <td>0.2631</td>\n",
       "      <td>21.339410</td>\n",
       "      <td>7.638983</td>\n",
       "      <td>13.6441</td>\n",
       "      <td>9.395263</td>\n",
       "      <td>4.189187</td>\n",
       "      <td>7.515655</td>\n",
       "      <td>-8.6673</td>\n",
       "      <td>19.4461</td>\n",
       "      <td>13.501060</td>\n",
       "      <td>4.3231</td>\n",
       "      <td>10.9173</td>\n",
       "      <td>11.9454</td>\n",
       "      <td>-0.6505</td>\n",
       "      <td>32.5443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.4204</td>\n",
       "      <td>23.7611</td>\n",
       "      <td>3.7186</td>\n",
       "      <td>0.755968</td>\n",
       "      <td>11.8779</td>\n",
       "      <td>-1.8234</td>\n",
       "      <td>5.920379</td>\n",
       "      <td>7.1458</td>\n",
       "      <td>15.060794</td>\n",
       "      <td>7.8786</td>\n",
       "      <td>28.0888</td>\n",
       "      <td>-1.195265</td>\n",
       "      <td>7.1993</td>\n",
       "      <td>11.1905</td>\n",
       "      <td>6.6116</td>\n",
       "      <td>-2.3597</td>\n",
       "      <td>-10.7189</td>\n",
       "      <td>23.174435</td>\n",
       "      <td>9.0707</td>\n",
       "      <td>-0.4787</td>\n",
       "      <td>10.7293</td>\n",
       "      <td>14.2577</td>\n",
       "      <td>9.5242</td>\n",
       "      <td>7.9807</td>\n",
       "      <td>9.740919</td>\n",
       "      <td>0.5706</td>\n",
       "      <td>5.5493</td>\n",
       "      <td>-2.793711</td>\n",
       "      <td>1.2804</td>\n",
       "      <td>16.6355</td>\n",
       "      <td>25.7454</td>\n",
       "      <td>6.8055</td>\n",
       "      <td>4.309451</td>\n",
       "      <td>14.7784</td>\n",
       "      <td>-15.5306</td>\n",
       "      <td>5.3785</td>\n",
       "      <td>11.8112</td>\n",
       "      <td>2.9926</td>\n",
       "      <td>15.619042</td>\n",
       "      <td>19.497495</td>\n",
       "      <td>...</td>\n",
       "      <td>3.808331</td>\n",
       "      <td>0.285369</td>\n",
       "      <td>5.640771</td>\n",
       "      <td>8.069448</td>\n",
       "      <td>16.1528</td>\n",
       "      <td>11.742000</td>\n",
       "      <td>17.1533</td>\n",
       "      <td>-3.912002</td>\n",
       "      <td>5.2175</td>\n",
       "      <td>15.886493</td>\n",
       "      <td>-0.624510</td>\n",
       "      <td>12.712798</td>\n",
       "      <td>3.2912</td>\n",
       "      <td>-2.243251</td>\n",
       "      <td>5.094744</td>\n",
       "      <td>1.8707</td>\n",
       "      <td>2.388877</td>\n",
       "      <td>43.2655</td>\n",
       "      <td>-0.5689</td>\n",
       "      <td>-3.8300</td>\n",
       "      <td>-1.0911</td>\n",
       "      <td>15.7108</td>\n",
       "      <td>11.9617</td>\n",
       "      <td>3.6810</td>\n",
       "      <td>-5.5623</td>\n",
       "      <td>-3.9611</td>\n",
       "      <td>23.242296</td>\n",
       "      <td>4.989016</td>\n",
       "      <td>12.6294</td>\n",
       "      <td>-0.168384</td>\n",
       "      <td>9.602282</td>\n",
       "      <td>5.956723</td>\n",
       "      <td>-13.1290</td>\n",
       "      <td>42.9089</td>\n",
       "      <td>10.181867</td>\n",
       "      <td>5.6157</td>\n",
       "      <td>11.1237</td>\n",
       "      <td>14.5033</td>\n",
       "      <td>10.0882</td>\n",
       "      <td>18.1030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89411</th>\n",
       "      <td>20.9624</td>\n",
       "      <td>12.0519</td>\n",
       "      <td>5.5867</td>\n",
       "      <td>0.239598</td>\n",
       "      <td>8.9262</td>\n",
       "      <td>1.2115</td>\n",
       "      <td>19.561262</td>\n",
       "      <td>9.5905</td>\n",
       "      <td>14.338467</td>\n",
       "      <td>3.9800</td>\n",
       "      <td>17.8846</td>\n",
       "      <td>3.081107</td>\n",
       "      <td>6.5837</td>\n",
       "      <td>19.1423</td>\n",
       "      <td>0.9047</td>\n",
       "      <td>-3.3969</td>\n",
       "      <td>-6.7519</td>\n",
       "      <td>25.894963</td>\n",
       "      <td>-12.1504</td>\n",
       "      <td>2.0688</td>\n",
       "      <td>11.5906</td>\n",
       "      <td>13.6435</td>\n",
       "      <td>17.4578</td>\n",
       "      <td>2.8207</td>\n",
       "      <td>8.375859</td>\n",
       "      <td>2.9819</td>\n",
       "      <td>11.8426</td>\n",
       "      <td>-0.033057</td>\n",
       "      <td>-0.9526</td>\n",
       "      <td>16.6386</td>\n",
       "      <td>21.6317</td>\n",
       "      <td>12.7435</td>\n",
       "      <td>-5.911038</td>\n",
       "      <td>8.9795</td>\n",
       "      <td>-5.5964</td>\n",
       "      <td>5.9838</td>\n",
       "      <td>15.1868</td>\n",
       "      <td>2.1215</td>\n",
       "      <td>17.355199</td>\n",
       "      <td>11.157408</td>\n",
       "      <td>...</td>\n",
       "      <td>6.251947</td>\n",
       "      <td>7.153173</td>\n",
       "      <td>6.018787</td>\n",
       "      <td>5.205671</td>\n",
       "      <td>-5.5622</td>\n",
       "      <td>5.626043</td>\n",
       "      <td>4.6441</td>\n",
       "      <td>-1.652246</td>\n",
       "      <td>-0.3287</td>\n",
       "      <td>17.549206</td>\n",
       "      <td>4.536351</td>\n",
       "      <td>8.200808</td>\n",
       "      <td>-2.8266</td>\n",
       "      <td>4.789231</td>\n",
       "      <td>16.145933</td>\n",
       "      <td>0.7481</td>\n",
       "      <td>3.292733</td>\n",
       "      <td>37.3572</td>\n",
       "      <td>-8.2003</td>\n",
       "      <td>-12.7438</td>\n",
       "      <td>1.5856</td>\n",
       "      <td>12.5841</td>\n",
       "      <td>11.7550</td>\n",
       "      <td>12.5825</td>\n",
       "      <td>4.1844</td>\n",
       "      <td>-4.3429</td>\n",
       "      <td>23.026939</td>\n",
       "      <td>5.554831</td>\n",
       "      <td>12.5434</td>\n",
       "      <td>6.027054</td>\n",
       "      <td>4.903006</td>\n",
       "      <td>2.031036</td>\n",
       "      <td>-31.0310</td>\n",
       "      <td>16.7117</td>\n",
       "      <td>19.659818</td>\n",
       "      <td>3.7358</td>\n",
       "      <td>11.2265</td>\n",
       "      <td>7.8009</td>\n",
       "      <td>9.0138</td>\n",
       "      <td>5.8544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89412</th>\n",
       "      <td>1.6635</td>\n",
       "      <td>13.0759</td>\n",
       "      <td>3.8412</td>\n",
       "      <td>0.157488</td>\n",
       "      <td>6.9696</td>\n",
       "      <td>-1.0613</td>\n",
       "      <td>4.420300</td>\n",
       "      <td>8.0782</td>\n",
       "      <td>19.859053</td>\n",
       "      <td>4.4054</td>\n",
       "      <td>24.1633</td>\n",
       "      <td>-8.450758</td>\n",
       "      <td>7.1593</td>\n",
       "      <td>22.6192</td>\n",
       "      <td>3.5584</td>\n",
       "      <td>2.4141</td>\n",
       "      <td>-6.9923</td>\n",
       "      <td>31.697447</td>\n",
       "      <td>-2.9128</td>\n",
       "      <td>2.1834</td>\n",
       "      <td>11.2383</td>\n",
       "      <td>14.0229</td>\n",
       "      <td>6.6065</td>\n",
       "      <td>6.9078</td>\n",
       "      <td>9.856049</td>\n",
       "      <td>-7.4198</td>\n",
       "      <td>5.6522</td>\n",
       "      <td>-0.983472</td>\n",
       "      <td>0.1544</td>\n",
       "      <td>14.8541</td>\n",
       "      <td>21.3090</td>\n",
       "      <td>9.0789</td>\n",
       "      <td>-5.134768</td>\n",
       "      <td>13.0412</td>\n",
       "      <td>-10.3088</td>\n",
       "      <td>5.8715</td>\n",
       "      <td>15.9882</td>\n",
       "      <td>-4.5244</td>\n",
       "      <td>25.825344</td>\n",
       "      <td>16.269394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.473681</td>\n",
       "      <td>4.158007</td>\n",
       "      <td>4.057913</td>\n",
       "      <td>-1.305373</td>\n",
       "      <td>-4.8469</td>\n",
       "      <td>4.894750</td>\n",
       "      <td>10.3075</td>\n",
       "      <td>-4.238404</td>\n",
       "      <td>6.2758</td>\n",
       "      <td>12.314248</td>\n",
       "      <td>3.606582</td>\n",
       "      <td>11.448829</td>\n",
       "      <td>0.1225</td>\n",
       "      <td>-1.382874</td>\n",
       "      <td>8.536330</td>\n",
       "      <td>2.5273</td>\n",
       "      <td>3.443137</td>\n",
       "      <td>29.8104</td>\n",
       "      <td>-21.2639</td>\n",
       "      <td>-4.5305</td>\n",
       "      <td>0.5135</td>\n",
       "      <td>15.4229</td>\n",
       "      <td>11.6878</td>\n",
       "      <td>7.2002</td>\n",
       "      <td>1.2748</td>\n",
       "      <td>-5.2950</td>\n",
       "      <td>24.988557</td>\n",
       "      <td>-1.491062</td>\n",
       "      <td>12.6617</td>\n",
       "      <td>7.302915</td>\n",
       "      <td>17.104629</td>\n",
       "      <td>9.761508</td>\n",
       "      <td>-27.6111</td>\n",
       "      <td>-1.7212</td>\n",
       "      <td>16.343360</td>\n",
       "      <td>9.5840</td>\n",
       "      <td>10.8084</td>\n",
       "      <td>10.5684</td>\n",
       "      <td>-3.0129</td>\n",
       "      <td>14.6587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89413</th>\n",
       "      <td>12.5236</td>\n",
       "      <td>24.9418</td>\n",
       "      <td>11.5404</td>\n",
       "      <td>0.092347</td>\n",
       "      <td>10.9600</td>\n",
       "      <td>-0.2548</td>\n",
       "      <td>9.902516</td>\n",
       "      <td>9.7123</td>\n",
       "      <td>17.893482</td>\n",
       "      <td>4.5450</td>\n",
       "      <td>37.8098</td>\n",
       "      <td>-0.246968</td>\n",
       "      <td>5.2791</td>\n",
       "      <td>15.3464</td>\n",
       "      <td>2.1507</td>\n",
       "      <td>-0.7455</td>\n",
       "      <td>-16.9012</td>\n",
       "      <td>21.759049</td>\n",
       "      <td>5.9232</td>\n",
       "      <td>2.7471</td>\n",
       "      <td>11.1839</td>\n",
       "      <td>13.9926</td>\n",
       "      <td>31.4700</td>\n",
       "      <td>10.8285</td>\n",
       "      <td>6.947698</td>\n",
       "      <td>5.5712</td>\n",
       "      <td>16.1846</td>\n",
       "      <td>-1.263778</td>\n",
       "      <td>3.8609</td>\n",
       "      <td>18.1579</td>\n",
       "      <td>24.0279</td>\n",
       "      <td>13.1059</td>\n",
       "      <td>-1.872695</td>\n",
       "      <td>7.1399</td>\n",
       "      <td>-10.0245</td>\n",
       "      <td>6.2477</td>\n",
       "      <td>16.7626</td>\n",
       "      <td>-6.3653</td>\n",
       "      <td>13.316258</td>\n",
       "      <td>21.798662</td>\n",
       "      <td>...</td>\n",
       "      <td>6.068942</td>\n",
       "      <td>-1.341827</td>\n",
       "      <td>10.599827</td>\n",
       "      <td>3.729400</td>\n",
       "      <td>7.3968</td>\n",
       "      <td>8.863187</td>\n",
       "      <td>11.0302</td>\n",
       "      <td>-2.473277</td>\n",
       "      <td>-0.9336</td>\n",
       "      <td>14.659605</td>\n",
       "      <td>2.285522</td>\n",
       "      <td>2.688732</td>\n",
       "      <td>-12.4074</td>\n",
       "      <td>0.172543</td>\n",
       "      <td>8.524252</td>\n",
       "      <td>-3.2063</td>\n",
       "      <td>3.628879</td>\n",
       "      <td>20.0940</td>\n",
       "      <td>-5.1950</td>\n",
       "      <td>-12.2145</td>\n",
       "      <td>1.4217</td>\n",
       "      <td>12.1684</td>\n",
       "      <td>11.1564</td>\n",
       "      <td>10.6035</td>\n",
       "      <td>-4.7392</td>\n",
       "      <td>-3.8501</td>\n",
       "      <td>28.323033</td>\n",
       "      <td>4.761864</td>\n",
       "      <td>12.9533</td>\n",
       "      <td>11.804298</td>\n",
       "      <td>12.852103</td>\n",
       "      <td>4.931833</td>\n",
       "      <td>-9.2469</td>\n",
       "      <td>11.9544</td>\n",
       "      <td>15.270576</td>\n",
       "      <td>6.7951</td>\n",
       "      <td>11.6992</td>\n",
       "      <td>11.6344</td>\n",
       "      <td>5.8713</td>\n",
       "      <td>29.0477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89414</th>\n",
       "      <td>3.7362</td>\n",
       "      <td>2.2806</td>\n",
       "      <td>5.1252</td>\n",
       "      <td>-0.388863</td>\n",
       "      <td>11.1461</td>\n",
       "      <td>-3.2853</td>\n",
       "      <td>5.256710</td>\n",
       "      <td>10.2491</td>\n",
       "      <td>7.331612</td>\n",
       "      <td>6.0761</td>\n",
       "      <td>20.1916</td>\n",
       "      <td>-5.185019</td>\n",
       "      <td>6.2957</td>\n",
       "      <td>18.7025</td>\n",
       "      <td>2.9502</td>\n",
       "      <td>-3.6960</td>\n",
       "      <td>-14.2818</td>\n",
       "      <td>28.586680</td>\n",
       "      <td>2.1343</td>\n",
       "      <td>-7.0157</td>\n",
       "      <td>12.5004</td>\n",
       "      <td>13.6056</td>\n",
       "      <td>8.5719</td>\n",
       "      <td>6.3164</td>\n",
       "      <td>8.393746</td>\n",
       "      <td>5.8242</td>\n",
       "      <td>-5.1344</td>\n",
       "      <td>-5.890902</td>\n",
       "      <td>2.0796</td>\n",
       "      <td>16.0592</td>\n",
       "      <td>16.0846</td>\n",
       "      <td>8.9505</td>\n",
       "      <td>4.763390</td>\n",
       "      <td>10.2936</td>\n",
       "      <td>-6.1191</td>\n",
       "      <td>6.2595</td>\n",
       "      <td>19.5619</td>\n",
       "      <td>-3.6776</td>\n",
       "      <td>21.894888</td>\n",
       "      <td>14.458738</td>\n",
       "      <td>...</td>\n",
       "      <td>8.854896</td>\n",
       "      <td>-1.761814</td>\n",
       "      <td>13.602779</td>\n",
       "      <td>3.658195</td>\n",
       "      <td>-6.0541</td>\n",
       "      <td>7.170329</td>\n",
       "      <td>10.3845</td>\n",
       "      <td>-3.420810</td>\n",
       "      <td>7.4823</td>\n",
       "      <td>13.105332</td>\n",
       "      <td>3.297413</td>\n",
       "      <td>10.316240</td>\n",
       "      <td>5.9573</td>\n",
       "      <td>-2.890636</td>\n",
       "      <td>23.216520</td>\n",
       "      <td>11.1506</td>\n",
       "      <td>3.180247</td>\n",
       "      <td>28.0291</td>\n",
       "      <td>-6.5844</td>\n",
       "      <td>-3.9577</td>\n",
       "      <td>-1.6791</td>\n",
       "      <td>22.8706</td>\n",
       "      <td>11.5015</td>\n",
       "      <td>12.5216</td>\n",
       "      <td>1.2549</td>\n",
       "      <td>-7.1724</td>\n",
       "      <td>18.178123</td>\n",
       "      <td>-2.078052</td>\n",
       "      <td>12.6616</td>\n",
       "      <td>7.701018</td>\n",
       "      <td>6.813868</td>\n",
       "      <td>10.776782</td>\n",
       "      <td>-20.5208</td>\n",
       "      <td>13.3465</td>\n",
       "      <td>19.282606</td>\n",
       "      <td>6.7640</td>\n",
       "      <td>10.5067</td>\n",
       "      <td>6.3111</td>\n",
       "      <td>-2.8103</td>\n",
       "      <td>11.0105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89415</th>\n",
       "      <td>6.0730</td>\n",
       "      <td>13.2655</td>\n",
       "      <td>10.4632</td>\n",
       "      <td>-1.748414</td>\n",
       "      <td>10.6244</td>\n",
       "      <td>-2.7946</td>\n",
       "      <td>13.656385</td>\n",
       "      <td>7.9378</td>\n",
       "      <td>17.085647</td>\n",
       "      <td>4.5577</td>\n",
       "      <td>29.5662</td>\n",
       "      <td>-1.573894</td>\n",
       "      <td>5.2121</td>\n",
       "      <td>22.3517</td>\n",
       "      <td>4.5891</td>\n",
       "      <td>6.6996</td>\n",
       "      <td>-6.5616</td>\n",
       "      <td>21.829854</td>\n",
       "      <td>9.2718</td>\n",
       "      <td>3.7147</td>\n",
       "      <td>10.6616</td>\n",
       "      <td>13.9939</td>\n",
       "      <td>12.5416</td>\n",
       "      <td>8.3359</td>\n",
       "      <td>11.217973</td>\n",
       "      <td>1.7074</td>\n",
       "      <td>0.7734</td>\n",
       "      <td>-2.915915</td>\n",
       "      <td>0.4391</td>\n",
       "      <td>7.6656</td>\n",
       "      <td>15.7815</td>\n",
       "      <td>16.0691</td>\n",
       "      <td>-5.784661</td>\n",
       "      <td>10.0141</td>\n",
       "      <td>-11.3915</td>\n",
       "      <td>6.1825</td>\n",
       "      <td>18.2671</td>\n",
       "      <td>-7.5302</td>\n",
       "      <td>23.171657</td>\n",
       "      <td>15.105686</td>\n",
       "      <td>...</td>\n",
       "      <td>4.826444</td>\n",
       "      <td>0.476483</td>\n",
       "      <td>3.556378</td>\n",
       "      <td>5.150888</td>\n",
       "      <td>14.7467</td>\n",
       "      <td>11.584054</td>\n",
       "      <td>4.5144</td>\n",
       "      <td>-3.347678</td>\n",
       "      <td>1.3046</td>\n",
       "      <td>12.114142</td>\n",
       "      <td>6.450759</td>\n",
       "      <td>3.634439</td>\n",
       "      <td>-2.7261</td>\n",
       "      <td>-2.413387</td>\n",
       "      <td>8.680212</td>\n",
       "      <td>-11.0579</td>\n",
       "      <td>1.752822</td>\n",
       "      <td>47.4888</td>\n",
       "      <td>5.5378</td>\n",
       "      <td>-7.2985</td>\n",
       "      <td>-2.0081</td>\n",
       "      <td>24.9991</td>\n",
       "      <td>11.3715</td>\n",
       "      <td>8.9337</td>\n",
       "      <td>6.5238</td>\n",
       "      <td>-5.9086</td>\n",
       "      <td>13.119206</td>\n",
       "      <td>4.903295</td>\n",
       "      <td>12.2616</td>\n",
       "      <td>-3.012730</td>\n",
       "      <td>6.206819</td>\n",
       "      <td>2.008397</td>\n",
       "      <td>-16.0398</td>\n",
       "      <td>-3.8316</td>\n",
       "      <td>10.342515</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>12.0632</td>\n",
       "      <td>10.6917</td>\n",
       "      <td>0.6781</td>\n",
       "      <td>27.1135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89416 rows × 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        feat_0   feat_1   feat_2  ...  feat_144  feat_145  feat_146\n",
       "0      15.1368  22.0563  13.8909  ...   10.5976    2.4412    7.8157\n",
       "1      17.8086   6.7716   7.6309  ...   13.4864    5.5657   34.4324\n",
       "2      16.6050   8.9661   5.2370  ...   14.1878    0.8427   14.5175\n",
       "3      17.9498  21.0946  11.8534  ...   11.9454   -0.6505   32.5443\n",
       "4      11.4204  23.7611   3.7186  ...   14.5033   10.0882   18.1030\n",
       "...        ...      ...      ...  ...       ...       ...       ...\n",
       "89411  20.9624  12.0519   5.5867  ...    7.8009    9.0138    5.8544\n",
       "89412   1.6635  13.0759   3.8412  ...   10.5684   -3.0129   14.6587\n",
       "89413  12.5236  24.9418  11.5404  ...   11.6344    5.8713   29.0477\n",
       "89414   3.7362   2.2806   5.1252  ...    6.3111   -2.8103   11.0105\n",
       "89415   6.0730  13.2655  10.4632  ...   10.6917    0.6781   27.1135\n",
       "\n",
       "[89416 rows x 119 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "LWURggC5pBNc",
    "outputId": "e8ce31e0-fe48-4119-8758-58ccda5b2f69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.6/dist-packages (0.4.3)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (0.22.2.post1)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.18.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->imbalanced-learn) (0.15.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn\n",
    "import imblearn\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "l2o9rbCLr8sK",
    "outputId": "f56a6665-76b1-4b20-b860-dc6c49b5f598"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'target': 1})\n"
     ]
    }
   ],
   "source": [
    "counter = Counter(YTrain)\n",
    "\n",
    "print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "qskQhtCfjhl5",
    "outputId": "040659e1-1d17-435b-a30d-ac454b57ce7f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#undersample = NearMiss(version=3, n_neighbors_ver3=3)\n",
    "#XTrain, YTrain = undersample.fit_resample(XTrain, YTrain)\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "XTraining, XValidation, YTraining, YValidation = train_test_split(XTrain,YTrain, test_size=0.3) # before model building\n",
    "\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "XTraining, YTraining = rus.fit_resample(XTraining, YTraining)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "0n8gMhnIsCDd",
    "outputId": "3ff3e1c3-80c9-4ce9-c2f7-fdf86821ac20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26825, 119)\n",
      "(26825, 1)\n",
      "(12566, 119)\n",
      "(12566, 1)\n"
     ]
    }
   ],
   "source": [
    "print(XValidation.shape)\n",
    "print(YValidation.shape)\n",
    "print(XTraining.shape)\n",
    "print(YTraining.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "W39ory3Wt2TA",
    "outputId": "2a390dec-e1c0-4a03-aa2b-9322b6875b4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44042, 119)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "XTraining = scaler.fit_transform(XTraining)\n",
    "XValidation = scaler.transform(XValidation)\n",
    "XTest = scaler.transform(XTest)\n",
    "print(XTest.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "ZdR3dsz4xfK5",
    "outputId": "fa0911bb-8e38-4fa1-e812-3ea4b67c1f54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.01, 'penalty': 'l2'}\n",
      "F1:\n",
      "[0.79596881 0.28993737]\n",
      "(44042,)\n",
      "[1 1 0 ... 0 0 0]\n",
      "          ID  target\n",
      "0          0       1\n",
      "1          1       1\n",
      "2          2       0\n",
      "3          3       0\n",
      "4          4       0\n",
      "...      ...     ...\n",
      "44037  44037       0\n",
      "44038  44038       1\n",
      "44039  44039       0\n",
      "44040  44040       0\n",
      "44041  44041       0\n",
      "\n",
      "[44042 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#LOGISTIC REGRESSION:\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "LR = LogisticRegression()\n",
    "\n",
    "skf = StratifiedKFold(n_splits=20)\n",
    "\n",
    "grid = {\"C\":[0.0001,0.001,0.01,0.1,1,10,100,1000,10000], \n",
    "        \"penalty\":[\"l2\"]}\n",
    "\n",
    "logreg_cv = GridSearchCV(LR, cv=skf, param_grid=grid, scoring = \"f1\")\n",
    "\n",
    "\n",
    "logreg_cv.fit(XTraining, np.ravel(YTraining))\n",
    "\n",
    "final_model_RF = logreg_cv.best_estimator_\n",
    "\n",
    "print(logreg_cv.best_params_)\n",
    "\n",
    "y_pred = final_model_RF.predict(XValidation)\n",
    "\n",
    "f1_LR = f1_score(YValidation, y_pred, average=None)\n",
    "\n",
    "print(\"F1:\")\n",
    "print(f1_LR)\n",
    "\n",
    "results = final_model_RF.predict(XTest)\n",
    "\n",
    "print(results.shape)\n",
    "print(results)\n",
    "dataset = pd.DataFrame({'ID': np.arange(0,44042), 'target': results})\n",
    "print(dataset)\n",
    "dataset.to_csv('LRpredictions.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree (DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "colab_type": "code",
    "id": "l0qujaPe0Yfz",
    "outputId": "3646d7ba-75e3-4a38-eef2-f2bfe377fe9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:\n",
      "[0.69927378 0.20175737]\n",
      "(44042,)\n",
      "[1 1 0 ... 0 0 0]\n",
      "          ID  target\n",
      "0          0       1\n",
      "1          1       1\n",
      "2          2       0\n",
      "3          3       0\n",
      "4          4       1\n",
      "...      ...     ...\n",
      "44037  44037       0\n",
      "44038  44038       0\n",
      "44039  44039       0\n",
      "44040  44040       0\n",
      "44041  44041       0\n",
      "\n",
      "[44042 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "#grid = {'max_leaf_nodes': list(range(2, 100)), 'min_samples_split': [ 2, 3, 4, 5, 6, 7 ]}\n",
    "\n",
    "#DecTree_cv = GridSearchCV(decision_tree,grid,cv=5)\n",
    "\n",
    "\n",
    "decision_tree.fit(XTraining, YTraining)\n",
    "\n",
    "\n",
    "Y_pred_decision_tree = decision_tree.predict(XValidation)\n",
    "\n",
    "decision_tree_f1 =  f1_score(YValidation, Y_pred_decision_tree, average= None)\n",
    "\n",
    "print(\"F1:\")\n",
    "print(decision_tree_f1)\n",
    "\n",
    "results = decision_tree.predict(XTest)\n",
    "\n",
    "print(results.shape)\n",
    "print(results)\n",
    "dataset = pd.DataFrame({'ID': np.arange(0,44042), 'target': results})\n",
    "print(dataset)\n",
    "dataset.to_csv('DTpredictions.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "Yq1ZYrm_0ksm",
    "outputId": "3613d8b6-6cd5-4ee5-ce5b-a4adef063d5d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:\n",
      "[0.7964415  0.28944505]\n",
      "(44042,)\n",
      "[1 1 0 ... 0 1 0]\n",
      "          ID  target\n",
      "0          0       1\n",
      "1          1       1\n",
      "2          2       0\n",
      "3          3       0\n",
      "4          4       0\n",
      "...      ...     ...\n",
      "44037  44037       0\n",
      "44038  44038       1\n",
      "44039  44039       0\n",
      "44040  44040       1\n",
      "44041  44041       0\n",
      "\n",
      "[44042 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "LDA = LinearDiscriminantAnalysis()\n",
    "\n",
    "LDA.fit(XTraining, YTraining)\n",
    "\n",
    "Y_pred_LDA = LDA.predict(XValidation)\n",
    "\n",
    "LDA_f1 =  f1_score(YValidation, Y_pred_LDA, average= None)\n",
    "\n",
    "\n",
    "print(\"F1:\")\n",
    "print(LDA_f1)\n",
    "\n",
    "results = LDA.predict(XTest)\n",
    "\n",
    "print(results.shape)\n",
    "print(results)\n",
    "dataset = pd.DataFrame({'ID': np.arange(0,44042), 'target': results})\n",
    "print(dataset)\n",
    "\n",
    "dataset.to_csv('LDApredictions.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadratic Discriminant Analysis (QDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "001MYQVY0p_L",
    "outputId": "240d42fe-7512-45f3-a849-efbe6919ab0b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:\n",
      "[0.7409184  0.23884322]\n",
      "(44042,)\n",
      "[0 1 1 ... 0 0 0]\n",
      "          ID  target\n",
      "0          0       0\n",
      "1          1       1\n",
      "2          2       1\n",
      "3          3       1\n",
      "4          4       0\n",
      "...      ...     ...\n",
      "44037  44037       1\n",
      "44038  44038       0\n",
      "44039  44039       0\n",
      "44040  44040       0\n",
      "44041  44041       0\n",
      "\n",
      "[44042 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "QDA = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "QDA.fit(XTraining, YTraining)\n",
    "\n",
    "y_pred = QDA.predict(XValidation)\n",
    "\n",
    "f1_QDA =  f1_score(YValidation, y_pred, average=None)\n",
    "\n",
    "\n",
    "print(\"F1:\")\n",
    "print(f1_QDA)\n",
    "\n",
    "results = QDA.predict(XTest)\n",
    "\n",
    "print(results.shape)\n",
    "print(results)\n",
    "dataset = pd.DataFrame({'ID': np.arange(0,44042), 'target': results})\n",
    "print(dataset)\n",
    "\n",
    "dataset.to_csv('QDApredictions.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "wNrwDwG_0sbJ",
    "outputId": "4d1115af-4d9b-4230-a251-2ede3eb392d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:\n",
      "[0.78683931 0.2799086 ]\n",
      "(44042,)\n",
      "[0 0 0 ... 1 1 0]\n",
      "          ID  target\n",
      "0          0       0\n",
      "1          1       0\n",
      "2          2       0\n",
      "3          3       1\n",
      "4          4       0\n",
      "...      ...     ...\n",
      "44037  44037       0\n",
      "44038  44038       0\n",
      "44039  44039       1\n",
      "44040  44040       1\n",
      "44041  44041       0\n",
      "\n",
      "[44042 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "Grid_RandomForest = GridSearchCV( estimator = random_forest, param_grid=param_grid, cv= 5)\n",
    "\n",
    "\n",
    "random_forest.fit(XTraining, YTraining)\n",
    "\n",
    "\n",
    "y_pred = random_forest.predict(XValidation)\n",
    "\n",
    "f1_random_forest =  f1_score(YValidation, y_pred, average=None)\n",
    "\n",
    "\n",
    "print(\"F1:\")\n",
    "print(f1_random_forest)\n",
    "\n",
    "results = random_forest.predict(XTest)\n",
    "\n",
    "print(results.shape)\n",
    "print(results)\n",
    "dataset = pd.DataFrame({'ID': np.arange(0,44042), 'target': results})\n",
    "print(dataset)\n",
    "\n",
    "dataset.to_csv('RFpredictions.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "BmZZJLb60ywX",
    "outputId": "921f5fdd-a99f-4b65-c8ce-a2c67679a423"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:\n",
      "[0.81115079 0.19990239]\n",
      "(44042,)\n",
      "[0 0 1 ... 0 0 0]\n",
      "          ID  target\n",
      "0          0       0\n",
      "1          1       0\n",
      "2          2       1\n",
      "3          3       1\n",
      "4          4       0\n",
      "...      ...     ...\n",
      "44037  44037       0\n",
      "44038  44038       0\n",
      "44039  44039       0\n",
      "44040  44040       0\n",
      "44041  44041       0\n",
      "\n",
      "[44042 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "KNN = KNeighborsClassifier()\n",
    "\n",
    "\n",
    "#parameters = {'n_neighbors':[3,4,5,6,7,10],\n",
    " #              'weights' : ['uniform','distance']\n",
    "  #            'leaf_size':[1,3,5],\n",
    "   #           'algorithm':['auto', 'kd_tree']}\n",
    "\n",
    "\n",
    "#Grid_KNN = GridSearchCV( estimator = KNN, param_grid=parameters, cv= 5)\n",
    "\n",
    "\n",
    "KNN.fit(XTraining, YTraining)\n",
    "\n",
    "y_pred = KNN.predict(XValidation)\n",
    "\n",
    "f1_score_KNN =  f1_score(YValidation, y_pred, average=None)\n",
    "\n",
    "print(\"F1:\")\n",
    "print(f1_score_KNN)\n",
    "\n",
    "results = KNN.predict(XTest)\n",
    "\n",
    "print(results.shape)\n",
    "print(results)\n",
    "dataset = pd.DataFrame({'ID': np.arange(0,44042), 'target': results})\n",
    "print(dataset)\n",
    "\n",
    "dataset.to_csv('KNNpredictions.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "colab_type": "code",
    "id": "wXjYVKBH02_k",
    "outputId": "e3f3e605-dd05-4f89-919a-c5f4556df6be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:\n",
      "[0.81090193 0.30913349]\n",
      "[1 1 0 ... 0 0 0]\n",
      "(44042,)\n",
      "          ID  target\n",
      "0          0       1\n",
      "1          1       1\n",
      "2          2       0\n",
      "3          3       0\n",
      "4          4       0\n",
      "...      ...     ...\n",
      "44037  44037       0\n",
      "44038  44038       0\n",
      "44039  44039       0\n",
      "44040  44040       0\n",
      "44041  44041       0\n",
      "\n",
      "[44042 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "bayes = GaussianNB()\n",
    "\n",
    "skf = StratifiedKFold(n_splits=20)\n",
    "\n",
    "bayes = GridSearchCV(bayes, cv=skf, param_grid={}, scoring = \"f1\")\n",
    "\n",
    "bayes.fit(XTraining, np.ravel(YTraining))\n",
    "\n",
    "y_pred_NB = bayes.predict(XValidation)\n",
    "\n",
    "f1_NB = f1_score(YValidation, y_pred_NB, average=None)\n",
    "\n",
    "print(\"F1:\")\n",
    "print(f1_NB)\n",
    "\n",
    "results = bayes.predict(XTest)\n",
    "print(results)\n",
    "print(results.shape)\n",
    "dataset = pd.DataFrame({'ID': np.arange(0,44042), 'target': results.astype(int)})\n",
    "print(dataset)\n",
    "dataset.to_csv('GNBpredictions.csv', index=False, sep=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "B687cBdn1EEt",
    "outputId": "57677a77-2782-464f-c67a-b1cb06d9d1ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:\n",
      "[0.8259408 0.3105234]\n"
     ]
    }
   ],
   "source": [
    "# Create Learners per layer\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "layer_one_estimators = [\n",
    "                        ('random_forest', RandomForestClassifier()),\n",
    "                        ('LDA', LinearDiscriminantAnalysis()),\n",
    "                        ('bayes', GaussianNB())\n",
    "                        \n",
    "                        \n",
    "                       ]\n",
    "layer_two_estimators = [\n",
    "                        ('xgb', XGBClassifier( n_estimators=600, objective='binary:logistic', silent=True, nthread=1,\n",
    "                                    subsample = 0.8, min_child_weight = 10,  max_depth = 5, learning_rate = 0.02,\n",
    "                                    gamma = 1, colsample_bytree =1\n",
    "                        ))\n",
    "                       ]\n",
    "\n",
    "\n",
    "\n",
    "layer_two = StackingClassifier(estimators=layer_two_estimators, final_estimator=LogisticRegression(C=0.1))\n",
    "\n",
    "# Create Final model by \n",
    "grid_final = StackingClassifier(estimators=layer_one_estimators, final_estimator=layer_two)\n",
    "\n",
    "\n",
    "grid_final.fit(XTraining, np.ravel(YTraining))\n",
    "\n",
    "y_pred_mix = grid_final.predict(XValidation)\n",
    "\n",
    "f1_mix = f1_score(YValidation, y_pred_mix, average=None)\n",
    "\n",
    "print(\"F1:\")\n",
    "print(f1_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "OUnHh0nsH__Q",
    "outputId": "3360285f-6453-4791-d0d4-5ddd7b5f796d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:\n",
      "[0.79837971 0.31323916]\n"
     ]
    }
   ],
   "source": [
    "layer_one_estimators = [\n",
    "                          ('LDA', LinearDiscriminantAnalysis()),\n",
    "                          ('random_forest', RandomForestClassifier(random_state = 51)),\n",
    "                          ('bayes', GaussianNB())\n",
    "                       ]\n",
    "\n",
    "layer_two_estimators = [\n",
    "                        ('xgb', XGBClassifier( n_estimators=600, objective='binary:logistic', silent=True, nthread=1,\n",
    "                                    subsample = 0.8, min_child_weight = 10,  max_depth = 5, learning_rate = 0.02,\n",
    "                                    gamma = 1, colsample_bytree =1, random_state = 51\n",
    "                        ))\n",
    "                       ]\n",
    "\n",
    "\n",
    "layer_two = StackingClassifier(estimators=layer_two_estimators, final_estimator=LogisticRegression())\n",
    "\n",
    "# Create Final model by \n",
    "grid_final = StackingClassifier(estimators=layer_one_estimators, final_estimator=layer_two)\n",
    "\n",
    "\n",
    "grid_final.fit(XTraining, np.ravel(YTraining))\n",
    "\n",
    "y_pred_mix = grid_final.predict(XValidation)\n",
    "\n",
    "f1_mix = f1_score(YValidation, y_pred_mix, average=None)\n",
    "\n",
    "print(\"F1:\")\n",
    "print(f1_mix)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Competition_1_badbunnybabe_1_FERNANDO_ULTIMATE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
